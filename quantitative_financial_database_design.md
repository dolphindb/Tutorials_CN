# DolphinDB教程：量化金融数据库设计最佳实践

DolphinDB的每一个数据库都采用一种分区机制（partitioning scheme）。一个数据库内的所有事实表（fact table）共享这种分区机制，而且可以保证同一个分区的多个子表（tablet chunk）数据落在同一个节点上，这样多个事实表（譬如quotes和trades）join效率非常高。DolphinDB作为一个列式分析型数据库，宽表是最常见的使用场景。一个应用一般只会用到宽表中的某几个列，其余没有用到的列，无论数量多少，不会影响数据库性能。因此可以把采集频率相同的数据放到一个宽表中，从而简化数据库的设计。

除了分布式的事实表，DolphinDB还提供了不分区的维度表（dimensional table）。维度表通常用于存储reference data。这类数据的数据量通常不会随着时间的积累而增长，而且数据内容变化较小。维度表可与采用任何分区机制的事实表进行join操作。DolphinDB还提供了基于内存和磁盘的流数据表，用于处理实时数据或者基于历史数据仿真的流数据。实时流数据表既可用于实时数据的快速查询，也可用于计算和监控，譬如K线计算和风险控制。仿真的流数据表可用于策略回测。

DolphinDB不仅是一个分布式数据库，更是一个强大的计算引擎。DolphinDB提供了多种内存表辅助计算和缓存。

## 1. 行情数据（market data）库的设计

### 1.1 行情数据库的分区设计

行情数据是量化金融中量级最大的数据类别。在中国的证券市场，累积的历史数据在20~40T左右，每日新增的数据在20~40G左右。若使用传统的关系型数据库来处理这些数据，性能极差。即使分库分表，性能也无法满足需要。

使用DolphinDB的分区机制，可以轻松应对100T甚至PB级别的数据量。行情数据通常可用时间和交易标的这两个维度来进行分区。时间维度大部分情况下可按天进行值分区。但是如果时间跨度不是很长，而单位时间内的数据量又非常大，也可以考虑按照小时进行分区。具体选择取决于具体的应用场景。譬如说每次的请求都是对单一股票进行查询或聚合计算，而且跨越的时间比较长，可能几个月甚至一年。那么时间维度上按月分区不失为一种好的做法。

产品标识维度的分区可以采用哈希，范围，值，列表等多种方法。如果每个标识在固定时间内的数据量比较均匀，哈希和范围都是可行的分区方法，如果产品的个数比较少，譬如期货的品种比较少，也可以考虑用值分区。中国的期货，股票市场都是以固定时间发布报价和交易的快照，因此不同产品的数据量分布非常均匀，可采用哈希或范围分区。美国金融市场的行情数据的分布则完全不同，不同股票的高频数据量差异非常大。这种情境下，建议采用范围分区，以一天或多天的数据为样本，将交易标的划分成多个范围，使得每一个范围内的产品的数据总量比较均衡。

DolphinDB中的多个分区维度并不是层级关系，而是平级的组合关系。如果时间维度有n个分区，产品维度有m个分区，最多可能有n*m个分区。DolphinDB在解决海量数据的存取时，并不提供行级的索引，而是将分区作为数据库的物理索引。因此每个分区的数据量不宜过大。一般我们建议每个分区压缩前的数据量在100MB左右。

行情数据包括日级数据，Level 1，Level 2，Level 3等不同级别的数据。不同级别的数据，数据量差异比较大。所以建议用不同的数据库（分区机制）来存储这些数据。

level1数据库设计：

level2数据库设计：

level3数据库设计：

### 1.2 不同资产的数据库设计

股票，期货和期权是金融市场中最常见的资产类别。是否把这些不同资产的行情数据存储在同一个数据库，甚至同一个表？这是量化金融数据库设计经常碰到的问题。我们建议设计不同的数据库来存储不同资产种类的数据。首先，时间维度上，单位时间内不同资产的数据量差别可能很大。其次，交易标的这个维度上，不同资产类别的交易标的数量差别也较大，很难用同一种方法进行分区。

不同资产采用不同数据库的一个潜在问题是如何实现不同资产数据之间的快速join。譬如我们在处理股票期权数据时，可能需要找到对应的股票的价格。通常可以先查询某一部分股票在某个时间范围内的行情数据，并存储于内存表；然后再将内存表跟期权价格的事实表进行关联。

### 1.3 K线数据库的设计

K线数据或相关的signal数据都是基于高精度的行情数据降低时间精度延伸出来的数据。通常，我们会生成不同频率的K线，譬如1分钟，5分钟，10分钟，30分钟，60分钟等等。这些不同频率的K线数据，因为数据量不是太大，建议存储在同一个分区表中。可以增加一个字段frequency来区分不同的时间窗口。K线表通常也按照日期和交易标的两个维度来分区。分区的粒度根据数据量来决定。以中国股票市场的分钟级K线为例。3000个股票每天产生约400个数据点，总共约120万个数据点。建议时间维度按月进行分区，产品的维度按范围或哈希分成30个分区。这样每个分区的数据量在100万行左右。这样的分区方法，既可以照顾到较长时间范围内（1个月或1年）快速查找某一个股票的数据，也可以兼顾到查找一天内全部股票的数据这样的问题。

### 1.4 行情数据的实时计算

DolphinDB内置了流数据处理引擎。行情数据可以发布到流数据表上，并被DolphinDB的数据节点或第三方的API（Python，Java， C++, Go等）订阅。订阅行情数据后，K线计算和风险控制等问题都可以快速实现。流数据的计算既可以用DolphinDB内置的计算引擎（时间序列聚合引擎，横截面聚合引擎等），也可以使用DolphinDB脚本或第三方API自定义消息处理函数。当使用DolphinDB脚本处理复杂的计算逻辑时，譬如计算Options的Greeks，可以使用启用JIT来提升计算速度。

- 流数据处理引擎相关教程请参考：[DolphinDB流数据教程](https://github.com/dolphindb/Tutorials_CN/blob/master/streaming_tutorial.md)、[DolphinDB时序聚合引擎教程](https://github.com/dolphindb/Tutorials_CN/blob/master/stream_aggregator.md)、[流数据横截面引擎](https://github.com/dolphindb/Tutorials_CN/blob/master/streaming_crossSectionalAggregator.md)等教程

- K线计算和风险控制的具体实现请参考：[DolphinDB K线生成指南](https://github.com/dolphindb/Tutorials_CN/blob/master/OHLC.md)

- JIT技术的具体使用示例请参考：[DolphinDB即时编译(JIT)教程](https://github.com/dolphindb/Tutorials_CN/blob/master/jit.md)

## 2. 周期性研究数据库的设计

量化金融的研究需要各种类型的数据。除了数据量最大的行情数据，还需要其它类型的数据，常用的包括宏观经济数据、上市公司的财务数据、分析师的预测数据以及上市公司的各类事件数据（除权，分割，分红等）。数据通常通常都会有时间戳（事件本身的时间戳和数据发布的时间戳）。这类数据相对于行情数据，数据量非常小，可以加载全部数据到内存中进行复杂的处理。

这类数据库的设计相对简单。如果数据量比较小，譬如宏观数据，直接存放到不分区的维度表就可以了。如果数据量比较大，可以考虑按照时间维度进行分区。周期性数据通常包含月度和季度数据，所以按照年度进行分区是一个比较可行的做法。如前所述，每一条记录通常包含两个时间戳，**事件本身的时间**和**数据发布的时间**。同一个事件有可能发布多次，通常后续的发布是对前面的数据的修正。因此，按时间进行维度分区时，一般选择事件本身的时间戳。

周期性研究数据的复杂性在于数据种类很多，数据来源很多。如何及时正确的同步外部数据源到DolphinDB数据库是一项非常重要的工作。我们可以借助一些数据同步中间件，根据数据发布的时间来完成数据同步。这儿我们重点讲一下数据库建模相关工作。前面提到周期性研究数据可能发布多个版本。我们面临两个选择，保留所有版本或者只保留最新版本。DolphinDB作为一个分布式的时序数据库，不支持单行数据的删除和更新操作。因此保留所有版本，对DolphinDB来说是最方便和高效。用户查询时，如果只需要最后一个版本，可以通过context by子句来实现。如果只保留最后一个版本，实现稍微复杂一点，需要以分区为单位，将分区数据加载到内存并更新，删除原有的分区数据，再将内存表数据写入到分区。

## 3. 参考数据（Reference Data）库的设计

数据仓库建模的雪花模型通常包括事实表和维度表。维度表的数据通常是事实表中的某些字段的外键。量化金融的数据库建模也会包含大量的维度数据，如每一个证券产品（如股票）的介绍（security master），参与交易的每一个机构（counterparty），每一种货币（currency），各类交易消息的编码(message)等。这一类数据在金融领域通常称为reference data。这一类数据数量非常有限，不会随着时间的变化线性增长。

DolphinDB提供了维度表（dimensional table）来存储reference data。维度表是不分区的，但是数据的写入和分区表无异。维度表可与任何分布式表，维度表以及内存表进行关联。维度表通常变化不会很频繁，DolphinDB自动会在各个节点上cache维度表，以提升查询和关联的性能。